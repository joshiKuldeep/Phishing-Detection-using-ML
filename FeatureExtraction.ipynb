{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDc1N9pTflmY"
   },
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3537,
     "status": "ok",
     "timestamp": 1638161621158,
     "user": {
      "displayName": "Kuldeep Joshi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiJ4WNwl1G6NPV39M0GVQrsKWORGyLmIPpbMlQSA_w=s64",
      "userId": "10194195710468067065"
     },
     "user_tz": -330
    },
    "id": "2vpD47_Nfkc9",
    "outputId": "a519ddb4-ce5f-4aae-8435-2acf0a128bd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-whois in c:\\users\\sys.ai\\anaconda3\\lib\\site-packages (0.7.3)\n",
      "Requirement already satisfied: future in c:\\users\\sys.ai\\anaconda3\\lib\\site-packages (from python-whois) (0.18.2)\n"
     ]
    }
   ],
   "source": [
    "# for quering and parsing of domain registration info.\n",
    "# LINK=> https://pypi.org/project/python-whois/\n",
    "!pip install python-whois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EXevmCydfbGY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sys.ai\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\sys.ai\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "C:\\Users\\sys.ai\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "# https://docs.python.org/3/library/ipaddress.html\n",
    "import ipaddress\n",
    "# https://docs.python.org/3/library/urllib.request.html\n",
    "import urllib.request\n",
    "from urllib.parse import urlparse, urlencode\n",
    "# disabling the urllib warnings\n",
    "import urllib3\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "# data scraping-> https://beautiful-soup-4.readthedocs.io/en/latest/\n",
    "from bs4 import BeautifulSoup\n",
    "import socket\n",
    "import requests\n",
    "# https://pypi.org/project/googlesearch-python/\n",
    "from googlesearch import search\n",
    "import whois\n",
    "from datetime import date, datetime\n",
    "import time\n",
    "from dateutil.parser import parse as date_parse\n",
    "\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "# data manipulation and analysis\n",
    "import pandas as pd\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WT-N-GaQlt3m"
   },
   "source": [
    "## Feature Extraction\n",
    "\n",
    "Now, going to extract the features which are explained in this [Paper](https://arxiv.org/pdf/2009.11116v1.pdf) and sub categorized as follows:\n",
    "\n",
    "1. Url based features\n",
    "\n",
    "2. DNS Records based features\n",
    "\n",
    "3. Page Content/ html based features \n",
    "\n",
    "* not all but some functions are taken from this [link](https://github.com/fafal-abnir/phishing_detection/blob/master/feature_extraction.py).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "m-LzH4_JM6V7"
   },
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "# https://stackoverflow.com/questions/44113335/extract-domain-from-url-in-python\n",
    "def _get_domain_from_url(url):\n",
    "  ''' function to get domain name from url '''\n",
    "  if not re.match(r\"^https?\", url): # checking if prefix like http and https are in url or not\n",
    "    url = \"http://\" + url           # if not, appending http:// string from starting in url to avoid schema issue\n",
    "\n",
    "  domain = urlparse(url).netloc\n",
    "  return domain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cAJCz0mt1Epr"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import requests\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# source: https://stackoverflow.com/a/68583332/5994461\n",
    "\n",
    "class requests_fetcher:\n",
    "  THREAD_POOL = 1024\n",
    "\n",
    "  # This is how to create a reusable connection pool with python requests.\n",
    "  session = requests.Session()\n",
    "  session.mount(\n",
    "      'https://',\n",
    "      requests.adapters.HTTPAdapter(pool_maxsize=THREAD_POOL,\n",
    "                                    max_retries=3,\n",
    "                                    pool_block=True)\n",
    "  )\n",
    "\n",
    "  def get_response(self, url, verify = False):\n",
    "    try:\n",
    "      self.response = self.session.get(url, timeout = 2, verify = verify)\n",
    "      # logging.info(\"request was completed in %s seconds [%s]\", response.elapsed.total_seconds(), response.url)\n",
    "      # if response.status_code != 200:\n",
    "      #     logging.error(\"request failed, error code %s [%s]\", response.status_code, response.url)\n",
    "      if 500 <= self.response.status_code < 600:\n",
    "          # server is overloaded? give it a break\n",
    "          time.sleep(5)\n",
    "\n",
    "      return self.response\n",
    "\n",
    "    except:\n",
    "      self.response = \"\"\n",
    "      return self.response\n",
    "\n",
    "  def download(self, urls, cl = None):\n",
    "      with ThreadPoolExecutor(max_workers=1024) as executor:\n",
    "          # wrap in a list() to wait for all requests to complete\n",
    "          if(cl):\n",
    "            ls = list(executor.map(cl.get_response, urls))\n",
    "          else:\n",
    "            ls = list(executor.map(self.get_response, urls))\n",
    "          return ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kdtRwKauuuo"
   },
   "source": [
    "### Url based features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRa4bJU_Ru--"
   },
   "source": [
    "**IP Address**\n",
    "\n",
    "Phishing-> If the IP address is used instead of domain name address.\n",
    "\n",
    "Legitimate-> Else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JPRu3UWejwDS"
   },
   "outputs": [],
   "source": [
    "def url_having_IP_Address(url):\n",
    "  try:\n",
    "    ipaddress.ip_address(url)\n",
    "    return 1\n",
    "  except:\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rm2t8krhSJNU"
   },
   "source": [
    "**Length**\n",
    "\n",
    "Phisher can use long URL to hide the query i.e. doubtful part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "I9geI1cnvO70"
   },
   "outputs": [],
   "source": [
    "def url_length(url):\n",
    "  return len(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nf_PBvjwu9fC"
   },
   "source": [
    "**Shortining Service**\n",
    "\n",
    "Links to the webpage that has a\n",
    "long URL. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "XehdUuOkvE7B"
   },
   "outputs": [],
   "source": [
    "shortening_services = r\"bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|\" \\\n",
    "                      r\"yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|\" \\\n",
    "                      r\"short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|\" \\\n",
    "                      r\"doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|db\\.tt|\" \\\n",
    "                      r\"qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|q\\.gs|is\\.gd|\" \\\n",
    "                      r\"po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|x\\.co|\" \\\n",
    "                      r\"prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|\" \\\n",
    "                      r\"tr\\.im|link\\.zip\\.net\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7o0QtlIswqCt"
   },
   "outputs": [],
   "source": [
    "def url_sortining_service(url):\n",
    "  match = re.search(shortening_services, url) # returns the span i.e starting and ending index of substring if available else returns none\n",
    "  return 1 if match else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpLEOsCocX5-"
   },
   "source": [
    "**Depth of url**\n",
    "\n",
    "Calculation of number of sub pages based on '/'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "S17Pi2ztcsDl"
   },
   "outputs": [],
   "source": [
    "def url_depth(url):\n",
    "  s = urlparse(url).path.split('/')\n",
    "  depth = 0\n",
    "  for j in range(len(s)):\n",
    "    if len(s[j]) != 0:\n",
    "      depth = depth+1\n",
    "  return depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tndLHkrXwAEw"
   },
   "source": [
    "**\"@\" Symbol**\n",
    "\n",
    "Phishing-> If @ symbol is in url\n",
    "\n",
    "Legitimate-> Else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "KSGYiO9qwOBu"
   },
   "outputs": [],
   "source": [
    "def url_having_at_symbol(url):\n",
    "  if \"@\" in url:\n",
    "    return 1\n",
    "  else:\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMcUIdyCw-WP"
   },
   "source": [
    "**\"//\" in URL**\n",
    "\n",
    "Used to redirect to another website.This comes under Redirection [attack](https://www.virtuesecurity.com/kb/url-redirection-attack-and-defense/).\n",
    "\n",
    "Phishing-> If \"//\" is in URL\n",
    "\n",
    "Legitimate-> Else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5x5S6sRZ2gtP"
   },
   "outputs": [],
   "source": [
    "def is_redirected(url):  \n",
    "  pos = url.rfind('//') \n",
    "  # Python string method rfind() returns the last index where the substring str is found, or -1 if no such index exists, optionally restricting the search to string[beg:end].\n",
    "  # ref. https://www.tutorialspoint.com/python/string_rfind.htm\n",
    "  return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "mydL8C7w4p59"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def top_frequent_values(urls, n):\n",
    "  counter = collections.Counter(urls)\n",
    "  print(counter)\n",
    "\n",
    "  phishing_counts = dict(counter.most_common(n))\n",
    "  print(\"Top five highest frequent values: \", phishing_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3oAVv7F3o1z"
   },
   "source": [
    "Analyzing what value to take in consideration for url_redirection measure which helps to distinguish between phishing and legitimate web site\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "JtdRRTgdQ8S9"
   },
   "outputs": [],
   "source": [
    "def url_redirection(url):\n",
    "  pos = url.rfind('//')\n",
    "  if pos >= 5:\n",
    "    return 1\n",
    "  else:\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QP-ycQEbxkE6"
   },
   "source": [
    "**Prefix Suffix**\n",
    "\n",
    "Phishers tend to add prefixes or suffixes separated by (-) to the domain name so that users feel that they are dealing with a legitimate website. For example http://www.Confirme-paypal.com\n",
    "\n",
    "Phishing-> If yes\n",
    "\n",
    "Legitimate-> Else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "bXVvUfTsxTc5"
   },
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "nt0LvVGuvXLZ"
   },
   "outputs": [],
   "source": [
    "def url_prefix_suffix(url):\n",
    "  if '-' in urlparse(url).netloc: # https://docs.python.org/3/library/urllib.parse.html(returns domain from url)\n",
    "    return 1\n",
    "  return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYjZFn2uvwnc"
   },
   "source": [
    "**HTTPS token**\n",
    "\n",
    "Phishing-> If url contains https token\n",
    "\n",
    "Legitimate-> Else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ngQSOUhMcFgq"
   },
   "outputs": [],
   "source": [
    "def url_http_domain(url):\n",
    "      if re.findall(r\"^https://\", url):\n",
    "        return 1\n",
    "      else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymeIvXtMvt2V"
   },
   "source": [
    "**Using non standard port**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "NFV0YAPEvwO1"
   },
   "outputs": [],
   "source": [
    "def url_standard_port(url):\n",
    "    try:\n",
    "        port = url.split(\":\")[1]\n",
    "        if port:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0 \n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3Zivvt2wURu"
   },
   "source": [
    "**Google Index**\n",
    "\n",
    "Checking google search wrt. url for n number of index\n",
    "\n",
    "If in any indexed link, phishing word is present -> Phishing\n",
    "\n",
    "Else -> Legitimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "SX6vBlZbt2cu"
   },
   "outputs": [],
   "source": [
    "def url_google_index(url):\n",
    "  try:\n",
    "    sites = list(search(query = url, stop = 5, user_agent= googlesearch.get_random_user_agent()))\n",
    "    for site in sites:\n",
    "      if('phishing' in site.lower()):\n",
    "        return 1\n",
    "    return 0\n",
    "  except:\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96lzLEBVPR_f"
   },
   "source": [
    "> **Creating a url_feature_extractor function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "y7DnQIpVPWho"
   },
   "outputs": [],
   "source": [
    "def url_based_feature_extractor(url):\n",
    "\n",
    "  features = []\n",
    "  # url based features\n",
    "  features.append(url)\n",
    "  features.append(url_having_IP_Address(url))\n",
    "  features.append(url_having_at_symbol(url))\n",
    "  features.append(url_length(url))\n",
    "  features.append(url_depth(url))\n",
    "  features.append(url_redirection(url))\n",
    "  features.append(url_http_domain(url))\n",
    "  features.append(url_sortining_service(url))\n",
    "  features.append(url_prefix_suffix(url))\n",
    "  features.append(url_standard_port(url))\n",
    "  features.append(url_google_index(url))\n",
    "\n",
    "  return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "G1NsxTXgPsd2"
   },
   "outputs": [],
   "source": [
    "# column names for dataframe \n",
    "url_col_names = ['url', 'url_having_IP_Address', 'url_having_at_symbol', 'url_length', 'url_depth', 'url_redirection', 'url_http_domain', 'url_sortining_service', 'url_prefix_suffix',\n",
    "             'url_standard_port', 'url_google_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_url_features(url):\n",
    "    url_features = []\n",
    "    if not re.match(r\"^https?\", url): # checking if prefix like http and https are in url or not\n",
    "     url = \"http://\" + url   \n",
    "    url_features.append(url_based_feature_extractor(url))\n",
    "    url_features_df = pd.DataFrame(url_features, columns = url_col_names)\n",
    "    return url_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yr5VVdKKvkHb"
   },
   "source": [
    "### DNS Record based features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIODypTbwNAA"
   },
   "source": [
    "**Age of Domain**\n",
    "\n",
    "This feature can be extracted from WHOIS database. Most phishing websites live for a short period of time. The minimum age of the legitimate domain is considered to be 6 months for this project. \n",
    "\n",
    "If age of domain < 6 months, the value of this feature is 1 (phishing) \n",
    "\n",
    "Else 0 (legitimate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "JeqylgicwOtD"
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "def domain_age(whois_response):\n",
    "    try:\n",
    "      registration_date = whois_response.creation_date \n",
    "      if(type(registration_date) == list): # if it is a list, fetching the first element of list as reg. date\n",
    "        registration_date = registration_date[0]\n",
    "\n",
    "      if abs((date.today() - registration_date.date()).days) >= 180:  #https://stackoverflow.com/questions/151199/how-to-calculate-number-of-days-between-two-given-dates\n",
    "        return 0\n",
    "      else:\n",
    "        return 1\n",
    "    except:\n",
    "      return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87K7jYIp8Wyt"
   },
   "source": [
    "\n",
    " **Domain Registration Length**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Tzn6sS6V8eS3"
   },
   "outputs": [],
   "source": [
    "# dissimilar to domain age, this feature uses expiration date \n",
    "\n",
    "def domain_registration_length(whois_reponse):\n",
    "  try:\n",
    "    expiration_date = whois_response.expiration_date \n",
    "    if(type(expiration_date) == list):\n",
    "     expiration_date = expiration_date[0]\n",
    "    registration_length = abs((expiration_date.date() - date.today()).days)\n",
    "    if registration_length / 365 <= 1:\n",
    "      return 1\n",
    "    else:\n",
    "      return 0\n",
    "  except:\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdJaTxWewY4j"
   },
   "source": [
    "**Statistical Report**\n",
    "\n",
    "If host belongs to ip_match i.e. top IPs selected based on statistical measures from Phishtank, it is Phishing\n",
    "\n",
    "Else-> Legitimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "2BHPQs2DlhEh"
   },
   "outputs": [],
   "source": [
    "def statistical_report(url, domain):\n",
    "    url_match = re.search(\n",
    "        'at\\.ua|usa\\.cc|baltazarpresentes\\.com\\.br|pe\\.hu|esy\\.es|hol\\.es|sweddy\\.com|myjino\\.ru|96\\.lt|ow\\.ly', url)\n",
    "    try:\n",
    "        ip_address = socket.gethostbyname(domain)\n",
    "        ip_match = re.search('146\\.112\\.61\\.108|213\\.174\\.157\\.151|121\\.50\\.168\\.88|192\\.185\\.217\\.116|78\\.46\\.211\\.158|181\\.174\\.165\\.13|46\\.242\\.145\\.103|121\\.50\\.168\\.40|83\\.125\\.22\\.219|46\\.242\\.145\\.98|'\n",
    "                             '107\\.151\\.148\\.44|107\\.151\\.148\\.107|64\\.70\\.19\\.203|199\\.184\\.144\\.27|107\\.151\\.148\\.108|107\\.151\\.148\\.109|119\\.28\\.52\\.61|54\\.83\\.43\\.69|52\\.69\\.166\\.231|216\\.58\\.192\\.225|'\n",
    "                             '118\\.184\\.25\\.86|67\\.208\\.74\\.71|23\\.253\\.126\\.58|104\\.239\\.157\\.210|175\\.126\\.123\\.219|141\\.8\\.224\\.221|10\\.10\\.10\\.10|43\\.229\\.108\\.32|103\\.232\\.215\\.140|69\\.172\\.201\\.153|'\n",
    "                             '216\\.218\\.185\\.162|54\\.225\\.104\\.146|103\\.243\\.24\\.98|199\\.59\\.243\\.120|31\\.170\\.160\\.61|213\\.19\\.128\\.77|62\\.113\\.226\\.131|208\\.100\\.26\\.234|195\\.16\\.127\\.102|195\\.16\\.127\\.157|'\n",
    "                             '34\\.196\\.13\\.28|103\\.224\\.212\\.222|172\\.217\\.4\\.225|54\\.72\\.9\\.51|192\\.64\\.147\\.141|198\\.200\\.56\\.183|23\\.253\\.164\\.103|52\\.48\\.191\\.26|52\\.214\\.197\\.72|87\\.98\\.255\\.18|209\\.99\\.17\\.27|'\n",
    "                             '216\\.38\\.62\\.18|104\\.130\\.124\\.96|47\\.89\\.58\\.141|78\\.46\\.211\\.158|54\\.86\\.225\\.156|54\\.82\\.156\\.19|37\\.157\\.192\\.102|204\\.11\\.56\\.48|110\\.34\\.231\\.42', ip_address)\n",
    "        if url_match:\n",
    "            return 1\n",
    "        elif ip_match:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mp9opV9tbpTX"
   },
   "source": [
    "> **Creating a domain_feature_extractor function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "iPwbNdfzw2nX"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import requests\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# source: https://stackoverflow.com/a/68583332/5994461\n",
    "\n",
    "THREAD_POOL = 1024\n",
    "\n",
    "  \n",
    "def get_domain_based_features(url):\n",
    "  features = []\n",
    "  # domain based features\n",
    "  dns_record = 0\n",
    "  try:\n",
    "    domain = _get_domain_from_url(url)\n",
    "    whois_response = whois.whois(domain)\n",
    "  except:\n",
    "    domain = ''\n",
    "    whois_response = ''\n",
    "    dns_record = 1\n",
    "\n",
    "  features.append(url)\n",
    "  features.append(dns_record)\n",
    "  features.append(1 if dns_record == 1 else domain_age(whois_response))\n",
    "  features.append(1 if dns_record == 1 else domain_registration_length(whois_response))\n",
    "  features.append(statistical_report(url, domain))\n",
    "  return features\n",
    "\n",
    "def domain_based_feature_extractor( urls):\n",
    "    with ThreadPoolExecutor(max_workers=THREAD_POOL) as executor:\n",
    "        # wrap in a list() to wait for all requests to complete\n",
    "        ls = list(executor.map(get_domain_based_features, urls))\n",
    "        return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 835,
     "status": "ok",
     "timestamp": 1638046612486,
     "user": {
      "displayName": "Kuldeep Joshi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiJ4WNwl1G6NPV39M0GVQrsKWORGyLmIPpbMlQSA_w=s64",
      "userId": "10194195710468067065"
     },
     "user_tz": -330
    },
    "id": "3eC6Hg1WaYII",
    "outputId": "5c34bb4c-bdeb-45cb-d63a-04d465365c46"
   },
   "outputs": [],
   "source": [
    "# column names for dataframe \n",
    "domain_col_names = ['url', 'dns_record', 'domain_age', 'domain_registration_length', 'statistical_report']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_domain_features(url):\n",
    "    domain_features = []\n",
    "    if not re.match(r\"^https?\", url): # checking if prefix like http and https are in url or not\n",
    "       url = \"http://\" + url   \n",
    "    domain_features.append(domain_based_feature_extractor([url]))\n",
    "    domain_features_df = pd.DataFrame(domain_features[0], columns = domain_col_names)\n",
    "    return domain_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xgv2OMZfeoKf"
   },
   "source": [
    "### Page content/ HTML based features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRNXTTc0wEUR"
   },
   "source": [
    "**Status bar customization(Mouse over)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "TZRpMrG1wGL5"
   },
   "outputs": [],
   "source": [
    "# checks the effect of mouse over on status bar (Mouse_Over)\n",
    "def _page_mouse_over(response): \n",
    "  if response == \"\" :\n",
    "    return 1\n",
    "  else:\n",
    "    if re.findall(\"<script>.+onmouseover.+</script>\", response.text):\n",
    "      return 1\n",
    "    else:\n",
    "      return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuTUmqbuwGwx"
   },
   "source": [
    "**Disabling Right Click**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "kbQ8u43PwIdJ"
   },
   "outputs": [],
   "source": [
    "def _page_right_click_disable(response):\n",
    "    if response == \"\":\n",
    "      return 1\n",
    "    else:\n",
    "     if re.findall(r\"event.button ?== ?2\", response.text):\n",
    "       return 0\n",
    "     else:\n",
    "       return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GecRJDPfvsB3"
   },
   "source": [
    "**Favicon**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0vqDtAYAvtYc"
   },
   "outputs": [],
   "source": [
    "def _page_favicon(soup, domain, url):  \n",
    "  if soup == -999:\n",
    "      return 1\n",
    "  else:\n",
    "      try:\n",
    "        for head in soup.find_all('head'):\n",
    "            for head.link in soup.find_all('link', href=True):\n",
    "                dots = [x.start(0)\n",
    "                        for x in re.finditer('\\.', head.link['href'])]\n",
    "                if url in head.link['href'] or len(dots) == 1 or domain in head.link['href']:\n",
    "                    return 0\n",
    "                else:\n",
    "                    return 1\n",
    "      except StopIteration:\n",
    "          pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-138l81wJBB"
   },
   "source": [
    "**Using pop-up window**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "51eHrg4RwLG2"
   },
   "outputs": [],
   "source": [
    " def _page_pop_up(response): \n",
    "    if response == \"\":\n",
    "        return 1\n",
    "    else:\n",
    "        if re.findall(r\"alert\\(\", response.text):\n",
    "            return 0\n",
    "        else:\n",
    "            return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zrnfLnkQwLcB"
   },
   "source": [
    "**IFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "kYBQoYEAwMhB"
   },
   "outputs": [],
   "source": [
    "def _page_iframe(response):\n",
    "  if response == \"\":\n",
    "    return 1\n",
    "  else:\n",
    "    if re.findall(r'[<iframe>|<frameBorder>]', response.text):\n",
    "      return 0\n",
    "    else:\n",
    "      return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7jOgT4-QhCZE"
   },
   "source": [
    "**Website forwarding**\n",
    "\n",
    "We find that legitimate websites have been redirected one time max. On the other hand, phishing websites containing this feature have been redirected at least 4 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "vHkkbBS5higj"
   },
   "outputs": [],
   "source": [
    "def _page_website_forwarding(response):\n",
    "  if response == \"\":\n",
    "    return 1\n",
    "  else:\n",
    "    if len(response.history) <= 2:\n",
    "      return 0\n",
    "    else:\n",
    "      return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sjqmrOOcwWBj"
   },
   "source": [
    "**Link pointing to the page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "d-tIvF2owYC8"
   },
   "outputs": [],
   "source": [
    "def _link_pointing_to_the_page(response):\n",
    "    if response == \"\":\n",
    "        return 1\n",
    "    else:\n",
    "        number_of_links = len(re.findall(r\"<a href=\", response.text))\n",
    "        if number_of_links == 0:\n",
    "            return 0\n",
    "        elif number_of_links <= 2:\n",
    "            return -1\n",
    "        else:\n",
    "           return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZSh-layv9og"
   },
   "source": [
    "**Submitting Information to email**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "peYQf8A6v_ch"
   },
   "outputs": [],
   "source": [
    "def _submit_email(response):\n",
    "  if response == \"\":\n",
    "        return 1\n",
    "  else:\n",
    "      if re.findall(r\"[mail\\(\\)|mailto:?]\", response.text):\n",
    "          return 1\n",
    "      else:\n",
    "          return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-MKOycGwBzr"
   },
   "source": [
    "**Website Redirection Count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "xR3vMjsSwD7p"
   },
   "outputs": [],
   "source": [
    "def _redirection_count(response):\n",
    "    if response == \"\":\n",
    "        return 1\n",
    "    else:\n",
    "        if len(response.history) <= 1: # https://www.geeksforgeeks.org/response-history-python-requests/\n",
    "            return 1\n",
    "        elif len(response.history) <= 4:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ihbC5URv7jl"
   },
   "source": [
    "**Server from Handler**\n",
    "\n",
    "If server_from_handler is_empty-> Phishing\n",
    "\n",
    "Elif it refers to a different domain-> Suspicious\n",
    "\n",
    "Else-> Legitimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "R7uAlDG_v9Fd"
   },
   "outputs": [],
   "source": [
    "def _server_from_handler(soup, domain, url):\n",
    "    if soup == -999:\n",
    "      return 1\n",
    "    elif len(soup.find_all('form', action=True))==0:\n",
    "      return 0\n",
    "    else :\n",
    "      for form in soup.find_all('form', action=True):\n",
    "          if form['action'] == \"\" or form['action'] == \"about:blank\":\n",
    "              return 1\n",
    "          elif url not in form['action'] and domain not in form['action']:\n",
    "              return -1\n",
    "          else:\n",
    "              return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fjf7j7DDvcbw"
   },
   "source": [
    "**SSL State**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "dNISx9NVvgBG"
   },
   "outputs": [],
   "source": [
    "def _ssl_state(response):\n",
    "  if response == \"\":\n",
    "    return 1\n",
    "  else:\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "998NHxuNbuLN"
   },
   "source": [
    "> **Creating a page_feature_extractor function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "dwyPB_uyAsrO"
   },
   "outputs": [],
   "source": [
    "class page_response_request(requests_fetcher):\n",
    "  curr_itr =0\n",
    "  def get_response(self, url):\n",
    "    self.response = super().get_response(url)\n",
    "    self.features = []\n",
    "    self.features.append(url)\n",
    "    self.page_features()\n",
    "\n",
    "    self.curr_itr += 1\n",
    "    if(self.curr_itr%500==0):\n",
    "      print('i: ', self.curr_itr)\n",
    "    return self.features\n",
    "\n",
    "  def page_features(self):\n",
    "    self.features.append(_page_mouse_over(self.response))\n",
    "    self.features.append(_page_right_click_disable(self.response))\n",
    "    self.features.append(_page_pop_up(self.response))\n",
    "    self.features.append(_page_iframe(self.response))\n",
    "    self.features.append(_page_website_forwarding(self.response))\n",
    "    self.features.append(_link_pointing_to_the_page(self.response))\n",
    "    self.features.append(_submit_email(self.response))\n",
    "    self.features.append(_redirection_count(self.response))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "a8ty6TTa-RH0"
   },
   "outputs": [],
   "source": [
    "page_col_names = ['url', 'page_mouse_over', 'page_right_click_disable', 'page_pop_up', 'page_iframe', 'page_website_forwarding', 'link_pointing_to_the_page', 'submit_email', \n",
    "             'redirection_count', 'server_from_handler','page_favicon',\t'ssl_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "jbcSnnMD-Msz"
   },
   "outputs": [],
   "source": [
    "# the features, urls get repeating when aligning with executor, creating a different method for this\n",
    "\n",
    "def soupbased(url):\n",
    "  req = requests_fetcher()\n",
    "  try:\n",
    "    response = req.get_response(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "  except:\n",
    "    soup = -999\n",
    "    response = \"\"\n",
    "\n",
    "  domain = _get_domain_from_url(url)\n",
    "  return [1 if soup == -999 else _server_from_handler(soup, domain = domain, url = url), 1 if soup == -999 else _page_favicon(soup, domain= domain, url = url), _ssl_state(response)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_requests_obj = page_response_request()\n",
    "\n",
    "def extract_page_features(url):\n",
    "    page_features = []\n",
    "    if not re.match(r\"^https?\", url): # checking if prefix like http and https are in url or not\n",
    "       url = \"http://\" + url   \n",
    "    page_features.append(page_requests_obj.get_response(url))\n",
    "    page_features[0]+= soupbased(url)\n",
    "    page_features_df = pd.DataFrame(page_features, columns = page_col_names)\n",
    "    return page_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YxGzF2_gmk4"
   },
   "source": [
    "### Generating features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_features(url):\n",
    "    url_df = extract_url_features(url)\n",
    "    domain_df = extract_domain_features(url)\n",
    "    page_df = extract_page_features(url)\n",
    "    df = url_df.merge(domain_df, on='url')\n",
    "    df = df.merge(page_df, on='url')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMY+2WR7EazAcI3Cr6DF0aP",
   "collapsed_sections": [],
   "name": "FeatureExtraction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
